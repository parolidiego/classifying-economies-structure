---
title: "Assignment I"
output: html_document
---

## Objective

TBW...

## Load libraries

```{r, message=FALSE}
rm(list = ls())
#install.packages("tidyverse")
#install.packages("DataExplorer")
#install.packages("mice")
#install.packages("ggplot2")
#install.packages("RColorBrewer")
#install.packages("explore")
#install.packages("corrplot")
#install.packages("factoextra")
library(tidyverse)
library(DataExplorer)
library(mice)
library(ggplot2)
library(RColorBrewer)
library(explore)
library(corrplot)
library(factoextra)
```

## Read the data

I have carefully handpicked variables that can help us characterize different economic structures from the World Development Indicators of the World Bank.

The dataset includes:

-   Value added of different sectors of the economy (as a % of GDP)

-   Trade balance of the country (as a % of GDP)

-   Trade openness (trade as % of GDP)

-   Central government debt (as a % of GDP) to capture public debt

-   Government consumption expenditure (as a % of GDP) to capture public spending

-   Unemployment (as a % of total labor force)

-   Youth unemployment (as a % of labor force 15-24 y.o.)

-   Net ODA received (as a % of GNI) to capture reliance on foreign aid

-   Total natural resources rents (as a % of GDP) to capture reliance on natural resources

-   International tourism, receipts (as a % of total exports) to capture reliance on tourism

-   \% of exports ascribable to different sectors to capture reliance on particular type of exports

    -   ex. foods exports as a % of total merchandise exports

    -   ex. insurance and financial services as a % of total commercial service exports

-   Labor force participation rate, female (as a % of total female population ages 15-64) to capture the involvement of women in the economy

-   International migrant stock (% of population) as a proxy of the importance of the foreign population

-   Age dependency ratio (% of people older than 64 to the total working-age population) as a proxy of the problem of an aging population

I downloaded data for years 2015 to 2019 so that by averaging over those years I am able to reduce the number of missing values but still retain a significant representation of the economic structure of different countries in a fairly recent period.

```{r}
raw_data <- read_csv("WDI_data.csv", show_col_types = FALSE)
head(raw_data)
```

Data can also be obtained the following way, directly from R by talking to the World Bank's API. If downloaded this way we would need to subsequently subset the dataframe, obtaining just the countries, as by running `WDI(country = "all"...)` we obtain data also for subnational geographical entities (ex. Africa Eastern and Southern etc.).

```{r}
# indicators_list <- raw_data |>
#   distinct(`Series Code`) |> 
#   drop_na() |>
#   pull(`Series Code`) 
# library(WDI)
# df = WDI(country = "all", indicator = indicators_list, start = 2015, end = 2019, extra = TRUE) 
```

## Cleaning and pre-processing

```{r, warning=FALSE}
data <- raw_data |> 
  # Dropping code of the indicator
  select(- "Series Code") |> 
  # Converting columns to numeric
  mutate(across(contains("YR"), as.numeric)) |> 
  # Obtaining the mean value for the 2015 to 2019 period
  mutate(mean_value = rowMeans(across(contains("YR")), na.rm = TRUE)) |> 
  # Dropping captions in the dataset
  slice(1:5208) |> 
  # Dropping columns with single year data
  select(-contains("YR")) |> 
  # Correctly encode NaN as NA
  mutate(mean_value = ifelse(is.nan(mean_value), NA, mean_value)) |>
  # Pivoting wider
  pivot_wider(names_from = "Series Name", values_from = "mean_value")

# Modifying column names
colnames(data) <- c(
  "country_name",  
  "country_code",  
  "agriculture_%gdp",  
  "industry_%gdp",  
  "services_%gdp",  
  "trade_balance",  
  "trade_openess",  
  "public_debt",  
  "public_spending",  
  "unemployment_total",  
  "unemployment_youth",  
  "foreign_aid",  
  "natural_resources_rents",  
  "tourism_exports",  
  "agriculture_exports",  
  "food_exports",  
  "fuel_exports",  
  "manufactures_exports",  
  "ores_metals_exports",  
  "ict_services_exports",  
  "finance_services_exports",  
  "transport_services_exports",  
  "travel_services_exports",  
  "female_labor_participation",  
  "migrant_population",  
  "age_dependency"
)
str(data)
```

```{r, fig.height=7}
# Check correlation
plot_correlation(data, type = "continuous", cor_args = list("use" = "pairwise.complete.obs"))
```

I confirm that the variables appear to be correlated at least to a certain extent and that therefore are appropriate for this assignment.

## Missing Data

```{r}
plot_intro(data)
```

First let's check if there are some countries with a particularly high number of missing data

```{r}
# Constructing a dataset counting how many missing data each country has 
missing_counts <- data.frame(
  country_name = data$country_name,
  missing_count = rowSums(is.na(data[, -c(1, 2)]))
)

# Extracting the countries with at least 50% of missing data (12 out of 24 variables)
high_missing_countries <- missing_counts |> 
  filter(missing_count >= 12) |> 
  pull(country_name)
high_missing_countries
```

All these countries are either small island countries or countries with a very close authoritarian regime or countries ravaged by wars. Therefore due to the high percentage of missing data and their peculiarity I decide to get rid of them.

```{r}
# Getting rid of them
reduced_df <- data |> 
  filter(!country_name %in% high_missing_countries)
```

I now plot the percentage of missing observations by column to see if some columns have a particularly low number of data.

```{r, fig.height=8}
plot_missing(data)
```

There is a worrying number of missing data especially for the variables `public_debt` and `foreign_aid`.

I now investigate more into the high percentage of missingness for the variable `foreign_aid`.

```{r}
# Extracting the list of countries that report NAs for foreign aid
reduced_df |> 
  filter(is.na(foreign_aid)) |> 
  pull(country_name)
```

All those countries are high-income countries (sometimes even donor countries) that do not receive official development assistance therefore I can safely recode these NAs into zeros.

```{r}
# Recoding foreign_aid NAs
reduced_df <- reduced_df |> 
  mutate(foreign_aid = ifelse(is.na(foreign_aid), 0, foreign_aid))
```

Finally I will now focus on the imputation of missing values for all variables, even for the column `public_debt`, that although it has a really high  number of missing values, I consider quite important for the scope of my analysis.

First I need to decide which imputation method to use. I take the variables with the most missing values (`public_debt` and `tourism_exports`) and impute them using several methods. By comparing the original distributions of these variables with those obtained through different imputation mechanisms I will select the "best" imputation algorithm and apply it throughout the whole dataset.

```{r, message=FALSE, warning=FALSE, results='hide'}
# Dropping for the imputation country_code and country_name (identifier variables)
numeric <- reduced_df |> 
  select(-c("country_name", "country_code"))
identifiers <- reduced_df |> 
  select(c("country_name", "country_code"))

# Imputing public_debt with several methods
imputed_public_debt <- data.frame(
  original = numeric$public_debt,
  imputed_pmm = complete(mice(numeric, m= 1, method = "pmm", seed=123))$public_debt,
  imputed_cart = complete(mice(numeric, m = 1, method = "cart", seed=123))$public_debt,
  imputed_rf = complete(mice(numeric, m = 1, method = "rf", seed=123))$public_debt,
  imputed_norm = complete(mice(numeric, m = 1, method = "norm", seed=123))$public_debt,
  imputed_norm.boot = complete(mice(numeric, m = 1, method = "norm.boot", seed=123))$public_debt,
  imputed_lasso.norm = complete(mice(numeric, m = 1, method = "lasso.norm", seed=123))$public_debt)

# Imputing tourism exports with several methods
imputed_tourism_exports <- data.frame(
  original = numeric$tourism_exports,
  imputed_pmm = complete(mice(numeric, m= 1, method = "pmm", seed=123))$tourism_exports,
  imputed_cart = complete(mice(numeric, m = 1, method = "cart", seed=123))$tourism_exports,
  imputed_rf = complete(mice(numeric, m = 1, method = "rf", seed=123))$tourism_exports,
  imputed_norm = complete(mice(numeric, m = 1, method = "norm", seed=123))$tourism_exports,
  imputed_norm.boot = complete(mice(numeric, m = 1, method = "norm.boot", seed=123))$tourism_exports,
  imputed_lasso.norm = complete(mice(numeric, m = 1, method = "lasso.norm", seed=123))$tourism_exports)
```

I will now plot the obtained distributions to understand which method best resembles the original distribution.

```{r, warning=FALSE}
# Plotting public_debt

# Convert data to long format for plotting
imputed_long_public_debt <- imputed_public_debt |> 
  pivot_longer(cols = everything(), names_to = "Method", values_to = "Value")

# Define color palette
colors_fill <- c(brewer.pal(6, "Set1"), "black")

# Plot with faceting
ggplot(imputed_long_public_debt, aes(x = Value, , fill = Method)) +
  geom_histogram(bins = 20, alpha = 0.7, color = "black") +
  facet_wrap(~Method, scales = "free_y") +
  scale_fill_manual(values = colors_fill) + 
  labs(title = "Comparison of Original and Imputed Distributions for Public Debt variable",
       x = "Public Debt",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")


# Plotting tourism exports

imputed_long_tourism <- imputed_tourism_exports %>%
  pivot_longer(cols = everything(), names_to = "Method", values_to = "Value")

colors_fill <- c(brewer.pal(6, "Set1"), "black")

ggplot(imputed_long_tourism, aes(x = Value, fill = Method)) +
  geom_histogram(bins = 20, alpha = 0.7, color = "black") +
  facet_wrap(~Method, scales = "free_y") +
  scale_fill_manual(values = colors_fill) + 
  labs(title = "Comparison of Original and Imputed Distributions for Tourism Exports Variable",
       x = "Tourism Exports",
       y = "Count") +
  theme_minimal() +
  theme(legend.position = "none") 
```

For both `public_debt` and `tourism_exports` predictive mean matching seems to be the algorithm that best resembles the original distribution so I will use it to predict the missing values for all the variables in the dataset.

```{r, results='hide', warning=FALSE}
imputed_df <- complete(mice(numeric, m= 1, method = "pmm", seed=123))
final_df <- cbind(identifiers, imputed_df)
```


## Descriptive analysis

```{r}
print(describe(final_df), n = 26)
```

As can be seen now there are no more missing values. All my variables are percentages, but they are percentages of different things (as explained when describing the data) and they can operate on different scales. For instance `trade_openess` is trade as a % of GDP, but it can and sometimes it takes values above 100%. The same can happen for `public_debt`. `trade_balance` instead can take negative values.

Here is where I decide whether I need to scale my variables or not... TBW...

```{r, fig.height=20}
#Plotting distribution with the mean
final_df |> 
  select(- c("country_name", "country_code")) |> 
  explore_all()
```

From the plots of the distributions of my variables I can see that most of them are right skewed, with many countries having values close to zero and long right tails. A partial exception to this are the variables `trade_balance` (because of its negative values) and `female_labor_participation` which exhibit partial left-skewness. More balanced are the distributions of `industry_%gdp`, `services_%gdp` and `travel_services_exports`, while `manufactures_exports` shows an interesting bimodal distribution.

```{r, fig.height=7, warning=FALSE}
final_df |> 
  select(- c("country_name", "country_code")) |> 
  cor() |> 
  corrplot(method = 'square', order = 'FPC', type = 'lower', diag = FALSE, tl.col = 'black')
```

Lastly I plot a correlation matrix from which it can be seen that there are some strong correlations, as well as numerous weak ones. For example having a high share of GDP coming from the primary sector (`agriculture_%gdp`) is negatively correlated with having a problem of an aging population (`age_dependency`), while it is positively correlated with receiving a lot of ODA (`foreign_aid`). 

## Principal Component Analysis

```{r}
pca = prcomp(imputed_df, scale = TRUE)
pca
```

As mentioned before, although all variables are percentages they are scaled to run principal components analysis to avoid the component weights being biased by the higher variance of some of the variables.

```{r}
fviz_screeplot(pca, addlabels = TRUE)
pca$rotation[,1]
fviz_contrib(pca, choice = "var", axes = 1)

# Convert PCA rotation values into a dataframe
pca_df <- data.frame(Variable = rownames(pca$rotation), 
                     PC1 = pca$rotation[,1])

# Create the barplot with rotated labels
ggplot(pca_df, aes(x = reorder(Variable, PC1), y = PC1)) +
  geom_bar(stat = "identity", fill = "darkblue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(x = "Variables", y = "PC1 Loadings", title = "PCA Loadings for PC1")


identifiers$country_name[order(pca$x[,1])][1:10]
identifiers$country_name[order(pca$x[,1], decreasing=T)][1:10]

data.frame(z1=pca$x[,1],z2=pca$x[,2]) |> 
  ggplot(aes(z1,z2,label=identifiers$country_name)) + 
  geom_point(size=0) +
  labs(title="First two principal components (scores)", x="PC1", y="PC2") + #guides(color=guide_legend(title="HDI"))+
  theme_bw() +theme(legend.position="bottom") + geom_text(size=3, hjust=0.6, vjust=0, check_overlap = TRUE) 

# Map our PCA index in a map:
map = data.frame(country=identifiers$country_name, country_code=identifiers$country_code, value=pca$x[,1])
# #Convert the country code into iso3c using the function countrycode()
# map$country = countrycode(map$country, 'country.name', 'iso3c')
#Create data object supporting the map
library(rworldmap)
matched <- joinCountryData2Map(map, joinCode = "ISO3",
                               nameJoinColumn = "country_code")
#Draw the map
mapCountryData(mapToPlot=matched,
               nameColumnToPlot="value",
               numCats = 6,
               missingCountryCol="white",
               addLegend = FALSE, # or true?
               borderCol = "black",
               catMethod = "diverging", 
               colourPalette = "diverging",
               mapTitle = c("PCA1"), 
               lwd=0.5,
               oceanCol="lightblue")
```

## Clustering

### K-Means

```{r, fig.height=10}
fviz_nbclust(scale(imputed_df), kmeans, method = 'wss', k.max = 10, nstart = 1000)
fviz_nbclust(scale(imputed_df), kmeans, method = 'silhouette', k.max = 10, nstart = 1000)
fviz_nbclust(scale(imputed_df), kmeans, method = 'gap_stat', k.max = 10, nstart = 100, nboot = 100, verbose = FALSE)

library(cluster)
library(mclust)
set.seed(123)

fit <- kmeans(scale(imputed_df), iter.max = 1000, centers = 8, nstart = 10000)
fit



# Number of observations by group
groups = fit$cluster
barplot(table(groups), col = "yellow")



# Extract centers data
centers_df <- as.data.frame(fit$centers)

# Add cluster labels
centers_df$Cluster <- factor(1:nrow(centers_df))  

# Convert to long format for ggplot
centers_df <- centers_df %>%
  pivot_longer(-Cluster, names_to = "Variable", values_to = "Value")

library(tidytext)
# Plot using ggplot
ggplot(centers_df, aes(x = reorder_within(Variable, Value, Cluster), 
                       y = Value, fill = Cluster)) +
  geom_col(position = "dodge") +
  facet_wrap(~ Cluster, scales = "free_x") +  
  labs(title = "Cluster Centers", x = "Variable", y = "Value") +
  scale_x_reordered() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Select here your favorite clustering tool
map = data.frame(country=identifiers$country_name, country_code=identifiers$country_code, value=fit$cluster)

#Create data object supporting the map
matched <- joinCountryData2Map(map, joinCode = "ISO3",
                               nameJoinColumn = "country_code")
#Draw the map
mapCountryData(mapToPlot=matched,
               nameColumnToPlot="value",
               numCats = 8,
               missingCountryCol="white",
               addLegend = TRUE, # or true?
               borderCol = "black",
               catMethod = "categorical", 
               colourPalette = "diverging",
               mapTitle = c("Clusters"), 
               lwd=0.5,
               oceanCol="lightblue")


fviz_cluster(fit, data = imputed_df, geom = c("point"), ellipse.type = 'norm', pointsize = 1)+
  theme_minimal()+
  geom_text(label = identifiers$country_name, hjust = 0, vjust = 0,size = 2,check_overlap = F)+
  scale_fill_brewer(palette = "Paired")
```

### Hierarchical

```{r}
d <- dist(scale(imputed_df), method = "euclidean") 
hc <- hclust(d, method = "ward.D2")

hc$labels <- identifiers$country_name

fviz_dend(x = hc, 
          k=8,
          palette = "jco", 
          rect = TRUE, rect_fill = TRUE, cex=0.5,
          rect_border = "jco"          
)

fviz_dend(x = hc,
          k = 8,
          color_labels_by_k = TRUE,
          cex = 0.8,
          type = "phylogenic",
          repel = TRUE)+  
  labs(title = "TBW...") + 
  theme(axis.text.x = element_blank(), axis.text.y = element_blank())



groups.hc = cutree(hc, k = 8)

# Map our PCA index in a map:
map = data.frame(country=identifiers$country_name, country_code=identifiers$country_code, value=groups.hc)
#Create data object supporting the map
matched <- joinCountryData2Map(map, joinCode = "ISO3",
                               nameJoinColumn = "country_code")
#Draw the map
mapCountryData(mapToPlot=matched,
               nameColumnToPlot="value",
               numCats = 8,
               missingCountryCol="white",
               addLegend = TRUE, # or true?
               borderCol = "black",
               catMethod = "categorical", 
               colourPalette = "diverging",
               mapTitle = c("Clusters"), 
               lwd=0.5,
               oceanCol="lightblue")
```

